{
  "summary": "Collects insert size distribution information on alignment data",
  "arguments": [
    {
      "summary": "If true, adds a command line header line to created VCF files.",
      "name": "--add-output-vcf-command-line",
      "synonyms": "-add-output-vcf-command-line",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "read one or more arguments files and add them to the command line",
      "name": "--arguments_file",
      "synonyms": "NA",
      "type": "List[File]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "maximum number of bytes to read from a file into each partition of reads. Setting this higher will result in fewer partitions. Note that this will not be equal to the size of the partition in memory. Defaults to 0, which uses the default split size (determined by the Hadoop input format, typically the size of one HDFS block).",
      "name": "--bam-partition-size",
      "synonyms": "NA",
      "type": "long",
      "required": "no",
      "fulltext": "",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Spark properties to set on the Spark context in the format \u003cproperty\u003e\u003d\u003cvalue\u003e",
      "name": "--conf",
      "synonyms": "NA",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "If true, create a BAM index when writing a coordinate-sorted BAM file.",
      "name": "--create-output-bam-index",
      "synonyms": "-OBI",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If true, create a BAM splitting index (SBI) when writing a coordinate-sorted BAM file.",
      "name": "--create-output-bam-splitting-index",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If true, create a VCF index when writing a coordinate-sorted VCF file.",
      "name": "--create-output-variant-index",
      "synonyms": "-OVI",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Read filters to be disabled before analysis",
      "name": "--disable-read-filter",
      "synonyms": "-DF",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If specified, do not check the sequence dictionaries from our inputs for compatibility. Use at your own risk!",
      "name": "--disable-sequence-dictionary-validation",
      "synonyms": "-disable-sequence-dictionary-validation",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Disable all tool default read filters (WARNING: many tools will not function correctly without their default read filters on)",
      "name": "--disable-tool-default-read-filters",
      "synonyms": "-disable-tool-default-read-filters",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "One or more genomic intervals to exclude from processing",
      "name": "--exclude-intervals",
      "synonyms": "-XL",
      "type": "List[String]",
      "required": "no",
      "fulltext": "Use this argument to exclude certain parts of the genome from the analysis (like -L, but the opposite).\n This argument can be specified multiple times. You can use samtools-style intervals either explicitly on the\n command line (e.g. -XL 1 or -XL 1:100-200) or by loading in a file containing a list of intervals\n (e.g. -XL myFile.intervals).",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "A configuration file to use with the GATK.",
      "name": "--gatk-config-file",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If the GCS bucket channel errors out, how many times it will attempt to re-initiate the connection",
      "name": "--gcs-max-retries",
      "synonyms": "-gcs-retries",
      "type": "int",
      "required": "no",
      "fulltext": "",
      "defaultValue": "20",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Project to bill when accessing \"requester pays\" buckets. If unset, these buckets cannot be accessed.  User must have storage.buckets.get permission on the bucket being accessed.",
      "name": "--gcs-project-for-requester-pays",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "\"\"",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "display the help message",
      "name": "--help",
      "synonyms": "-h",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Generate mean, sd and plots by trimming the data down to MEDIAN + maxMADTolerance*MEDIAN_ABSOLUTE_DEVIATION. This is done because insert size data typically includes enough anomalous values from chimeras and other artifacts to make the mean and sd grossly misleading regarding the real distribution.",
      "name": "--histogram-plot-deviations-tolerance",
      "synonyms": "-TOL",
      "type": "double",
      "required": "no",
      "fulltext": "",
      "defaultValue": "10.0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "File to write insert size histogram chart to.",
      "name": "--histogram-plot-file",
      "synonyms": "-H",
      "type": "String",
      "required": "yes",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "required",
      "options": []
    },
    {
      "summary": "BAM/SAM/CRAM file containing reads",
      "name": "--input",
      "synonyms": "-I",
      "type": "List[GATKPath]",
      "required": "yes",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "required",
      "options": []
    },
    {
      "summary": "Amount of padding (in bp) to add to each interval you are excluding.",
      "name": "--interval-exclusion-padding",
      "synonyms": "-ixp",
      "type": "int",
      "required": "no",
      "fulltext": "Use this to add padding to the intervals specified using -XL. For example, \u0027-XL 1:100\u0027 with a\n padding value of 20 would turn into \u0027-XL 1:80-120\u0027. This is typically used to add padding around targets when\n analyzing exomes.",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Interval merging rule for abutting intervals",
      "name": "--interval-merging-rule",
      "synonyms": "-imr",
      "type": "IntervalMergingRule",
      "required": "no",
      "fulltext": "By default, the program merges abutting intervals (i.e. intervals that are directly side-by-side but do not\n actually overlap) into a single continuous interval. However you can change this behavior if you want them to be\n treated as separate intervals instead.",
      "defaultValue": "ALL",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": [
        {
          "summary": "",
          "name": "ALL"
        },
        {
          "summary": "",
          "name": "OVERLAPPING_ONLY"
        }
      ]
    },
    {
      "summary": "Amount of padding (in bp) to add to each interval you are including.",
      "name": "--interval-padding",
      "synonyms": "-ip",
      "type": "int",
      "required": "no",
      "fulltext": "Use this to add padding to the intervals specified using -L. For example, \u0027-L 1:100\u0027 with a\n padding value of 20 would turn into \u0027-L 1:80-120\u0027. This is typically used to add padding around targets when\n analyzing exomes.",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Set merging approach to use for combining interval inputs",
      "name": "--interval-set-rule",
      "synonyms": "-isr",
      "type": "IntervalSetRule",
      "required": "no",
      "fulltext": "By default, the program will take the UNION of all intervals specified using -L and/or -XL. However, you can\n change this setting for -L, for example if you want to take the INTERSECTION of the sets instead. E.g. to\n perform the analysis only on chromosome 1 exomes, you could specify -L exomes.intervals -L 1 --interval-set-rule\n INTERSECTION. However, it is not possible to modify the merging approach for intervals passed using -XL (they will\n always be merged using UNION).\n\n Note that if you specify both -L and -XL, the -XL interval set will be subtracted from the -L interval set.",
      "defaultValue": "UNION",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": [
        {
          "summary": "Take the union of all intervals",
          "name": "UNION"
        },
        {
          "summary": "Take the intersection of intervals (the subset that overlaps all intervals specified)",
          "name": "INTERSECTION"
        }
      ]
    },
    {
      "summary": "One or more genomic intervals over which to operate",
      "name": "--intervals",
      "synonyms": "-L",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "The level(s) at which to accumulate metrics. Possible values are {ALL_READS, SAMPLE, LIBRARY, READ GROUP}.",
      "name": "--metric-accumulation-level",
      "synonyms": "-LEVEL",
      "type": "Set[MetricAccumulationLevel]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[ALL_READS]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": [
        {
          "summary": "",
          "name": "ALL_READS"
        },
        {
          "summary": "",
          "name": "SAMPLE"
        },
        {
          "summary": "",
          "name": "LIBRARY"
        },
        {
          "summary": "",
          "name": "READ_GROUP"
        }
      ]
    },
    {
      "summary": "When generating the histogram, discard any data categories (out of FR, TANDEM, RF) that have fewer than this percentage of overall reads (Range: 0 to 1).",
      "name": "--min-category-reads-percentage",
      "synonyms": "-M",
      "type": "float",
      "required": "no",
      "fulltext": "",
      "defaultValue": "0.05",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "For tools that shuffle data or write an output, sets the number of reducers. Defaults to 0, which gives one partition per 10MB of input.",
      "name": "--num-reducers",
      "synonyms": "NA",
      "type": "int",
      "required": "no",
      "fulltext": "",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "File to write the output to.",
      "name": "--output",
      "synonyms": "-O",
      "type": "String",
      "required": "yes",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "required",
      "options": []
    },
    {
      "summary": "when writing a bam, in single sharded mode this directory to write the temporary intermediate output shards, if not specified .parts/ will be used",
      "name": "--output-shard-tmp-dir",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "If enabled, an output .pdf plot will be created.",
      "name": "--produce-plot",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Name of the program running",
      "name": "--program-name",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Whether to suppress job-summary info on System.err.",
      "name": "--QUIET",
      "synonyms": "NA",
      "type": "Boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Read filters to be applied before analysis",
      "name": "--read-filter",
      "synonyms": "-RF",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Indices to use for the read inputs. If specified, an index must be provided for every read input and in the same order as the read inputs. If this argument is not specified, the path to the index for each input will be inferred automatically.",
      "name": "--read-index",
      "synonyms": "-read-index",
      "type": "List[GATKPath]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Validation stringency for all SAM/BAM/CRAM/SRA files read by this program.  The default stringency value SILENT can improve performance when processing a BAM file in which variable-length data (read, qualities, tags) do not otherwise need to be decoded.",
      "name": "--read-validation-stringency",
      "synonyms": "-VS",
      "type": "ValidationStringency",
      "required": "no",
      "fulltext": "",
      "defaultValue": "SILENT",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": [
        {
          "summary": "",
          "name": "STRICT"
        },
        {
          "summary": "",
          "name": "LENIENT"
        },
        {
          "summary": "",
          "name": "SILENT"
        }
      ]
    },
    {
      "summary": "Reference sequence",
      "name": "--reference",
      "synonyms": "-R",
      "type": "GATKPath",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "For tools that write an output, write the output in multiple pieces (shards)",
      "name": "--sharded-output",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "display hidden arguments",
      "name": "--showHidden",
      "synonyms": "-showHidden",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "advanced",
      "options": []
    },
    {
      "summary": "URL of the Spark Master to submit jobs to when using the Spark pipeline runner.",
      "name": "--spark-master",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "local[*]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Spark verbosity. Overrides --verbosity for Spark-generated logs only. Possible values: {ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF, TRACE}",
      "name": "--spark-verbosity",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Granularity to use when writing a splitting index, one entry will be put into the index every n reads where n is this granularity value. Smaller granularity results in a larger index with more available split points.",
      "name": "--splitting-index-granularity",
      "synonyms": "NA",
      "type": "long",
      "required": "no",
      "fulltext": "",
      "defaultValue": "4096",
      "minValue": "1.0",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Temp directory to use.",
      "name": "--tmp-dir",
      "synonyms": "NA",
      "type": "GATKPath",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Whether to use the JdkDeflater (as opposed to IntelDeflater)",
      "name": "--use-jdk-deflater",
      "synonyms": "-jdk-deflater",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Whether to use the JdkInflater (as opposed to IntelInflater)",
      "name": "--use-jdk-inflater",
      "synonyms": "-jdk-inflater",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Whether to use NIO or the Hadoop filesystem (default) for reading files. (Note that the Hadoop filesystem is always used for writing files.)",
      "name": "--use-nio",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Control verbosity of logging.",
      "name": "--verbosity",
      "synonyms": "-verbosity",
      "type": "LogLevel",
      "required": "no",
      "fulltext": "",
      "defaultValue": "INFO",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": [
        {
          "summary": "",
          "name": "ERROR"
        },
        {
          "summary": "",
          "name": "WARNING"
        },
        {
          "summary": "",
          "name": "INFO"
        },
        {
          "summary": "",
          "name": "DEBUG"
        }
      ]
    },
    {
      "summary": "display the version number for this tool",
      "name": "--version",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Explicitly sets the histogram width, overriding automatic truncation of Histogram tail. Also, when calculating mean and standard deviation, only bins \u003c\u003d HISTOGRAM_WIDTH will be included.",
      "name": "--width",
      "synonyms": "-W",
      "type": "Integer",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    }
  ],
  "description": "Collects insert size distribution information in alignment data. The tool leverages the Spark framework\n for faster operation.\n\n \u003ch3\u003eUsage example\u003c/h3\u003e\n \u003cpre\u003e\n gatk CollectInsertSizeMetricsSpark \\\n   -I gs://cloud-bucket/input.bam \\\n   -H gs://cloud-bucket/insert_size_histogram.pdf \\\n   -O gs://cloud-bucket/insert_size_metrics.txt \\\n   -- \\\n   --spark-runner GCS \\\n   --cluster my-dataproc-cluster\n \u003c/pre\u003e\n \u003cp\u003e\n See \u003ca href\u003dhttp://broadinstitute.github.io/picard/picard-metric-definitions.html#InsertSizeMetrics\u003e\n     http://broadinstitute.github.io/picard/picard-metric-definitions.html#InsertSizeMetrics\u003c/a\u003e\n for an explanation of individual metrics. See \u003ca href \u003d\"https://software.broadinstitute.org/gatk/documentation/article?id\u003d10060\"\u003e\n     Tutorial#10060\u003c/a\u003e for an example of how to set up and run a Spark tool on a cloud Spark cluster.\n \u003c/p\u003e",
  "name": "CollectInsertSizeMetricsSpark",
  "group": "Diagnostics and Quality Control",
  "beta": true,
  "experimental": false
}