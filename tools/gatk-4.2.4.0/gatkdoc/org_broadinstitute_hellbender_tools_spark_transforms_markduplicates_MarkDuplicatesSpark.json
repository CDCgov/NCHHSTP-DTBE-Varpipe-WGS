{
  "summary": "MarkDuplicates on Spark",
  "arguments": [
    {
      "summary": "If true, adds a command line header line to created VCF files.",
      "name": "--add-output-vcf-command-line",
      "synonyms": "-add-output-vcf-command-line",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Allow non-queryname sorted inputs when specifying multiple input bams.",
      "name": "--allow-multiple-sort-orders-in-input",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "advanced",
      "options": []
    },
    {
      "summary": "read one or more arguments files and add them to the command line",
      "name": "--arguments_file",
      "synonyms": "NA",
      "type": "List[File]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "maximum number of bytes to read from a file into each partition of reads. Setting this higher will result in fewer partitions. Note that this will not be equal to the size of the partition in memory. Defaults to 0, which uses the default split size (determined by the Hadoop input format, typically the size of one HDFS block).",
      "name": "--bam-partition-size",
      "synonyms": "NA",
      "type": "long",
      "required": "no",
      "fulltext": "",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Spark properties to set on the Spark context in the format \u003cproperty\u003e\u003d\u003cvalue\u003e",
      "name": "--conf",
      "synonyms": "NA",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "If true, create a BAM index when writing a coordinate-sorted BAM file.",
      "name": "--create-output-bam-index",
      "synonyms": "-OBI",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If true, create a BAM splitting index (SBI) when writing a coordinate-sorted BAM file.",
      "name": "--create-output-bam-splitting-index",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If true, create a VCF index when writing a coordinate-sorted VCF file.",
      "name": "--create-output-variant-index",
      "synonyms": "-OVI",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "true",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Read filters to be disabled before analysis",
      "name": "--disable-read-filter",
      "synonyms": "-DF",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If specified, do not check the sequence dictionaries from our inputs for compatibility. Use at your own risk!",
      "name": "--disable-sequence-dictionary-validation",
      "synonyms": "-disable-sequence-dictionary-validation",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Disable all tool default read filters (WARNING: many tools will not function correctly without their default read filters on)",
      "name": "--disable-tool-default-read-filters",
      "synonyms": "-disable-tool-default-read-filters",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Enabling this option will mean unmapped mates of duplicate marked reads will not be marked as duplicates.",
      "name": "--do-not-mark-unmapped-mates",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "The scoring strategy for choosing the non-duplicate among candidates.",
      "name": "--duplicate-scoring-strategy",
      "synonyms": "-DS",
      "type": "MarkDuplicatesScoringStrategy",
      "required": "no",
      "fulltext": "",
      "defaultValue": "SUM_OF_BASE_QUALITIES",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": [
        {
          "summary": "",
          "name": "SUM_OF_BASE_QUALITIES"
        },
        {
          "summary": "",
          "name": "TOTAL_MAPPED_REFERENCE_LENGTH"
        }
      ]
    },
    {
      "summary": "Determines how duplicate types are recorded in the DT optional attribute.",
      "name": "--duplicate-tagging-policy",
      "synonyms": "NA",
      "type": "DuplicateTaggingPolicy",
      "required": "no",
      "fulltext": "",
      "defaultValue": "DontTag",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": [
        {
          "summary": "",
          "name": "DontTag"
        },
        {
          "summary": "",
          "name": "OpticalOnly"
        },
        {
          "summary": "",
          "name": "All"
        }
      ]
    },
    {
      "summary": "One or more genomic intervals to exclude from processing",
      "name": "--exclude-intervals",
      "synonyms": "-XL",
      "type": "List[String]",
      "required": "no",
      "fulltext": "Use this argument to exclude certain parts of the genome from the analysis (like -L, but the opposite).\n This argument can be specified multiple times. You can use samtools-style intervals either explicitly on the\n command line (e.g. -XL 1 or -XL 1:100-200) or by loading in a file containing a list of intervals\n (e.g. -XL myFile.intervals).",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "A configuration file to use with the GATK.",
      "name": "--gatk-config-file",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "If the GCS bucket channel errors out, how many times it will attempt to re-initiate the connection",
      "name": "--gcs-max-retries",
      "synonyms": "-gcs-retries",
      "type": "int",
      "required": "no",
      "fulltext": "",
      "defaultValue": "20",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Project to bill when accessing \"requester pays\" buckets. If unset, these buckets cannot be accessed.  User must have storage.buckets.get permission on the bucket being accessed.",
      "name": "--gcs-project-for-requester-pays",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "\"\"",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "display the help message",
      "name": "--help",
      "synonyms": "-h",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "BAM/SAM/CRAM file containing reads",
      "name": "--input",
      "synonyms": "-I",
      "type": "List[GATKPath]",
      "required": "yes",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "required",
      "options": []
    },
    {
      "summary": "Amount of padding (in bp) to add to each interval you are excluding.",
      "name": "--interval-exclusion-padding",
      "synonyms": "-ixp",
      "type": "int",
      "required": "no",
      "fulltext": "Use this to add padding to the intervals specified using -XL. For example, \u0027-XL 1:100\u0027 with a\n padding value of 20 would turn into \u0027-XL 1:80-120\u0027. This is typically used to add padding around targets when\n analyzing exomes.",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Interval merging rule for abutting intervals",
      "name": "--interval-merging-rule",
      "synonyms": "-imr",
      "type": "IntervalMergingRule",
      "required": "no",
      "fulltext": "By default, the program merges abutting intervals (i.e. intervals that are directly side-by-side but do not\n actually overlap) into a single continuous interval. However you can change this behavior if you want them to be\n treated as separate intervals instead.",
      "defaultValue": "ALL",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": [
        {
          "summary": "",
          "name": "ALL"
        },
        {
          "summary": "",
          "name": "OVERLAPPING_ONLY"
        }
      ]
    },
    {
      "summary": "Amount of padding (in bp) to add to each interval you are including.",
      "name": "--interval-padding",
      "synonyms": "-ip",
      "type": "int",
      "required": "no",
      "fulltext": "Use this to add padding to the intervals specified using -L. For example, \u0027-L 1:100\u0027 with a\n padding value of 20 would turn into \u0027-L 1:80-120\u0027. This is typically used to add padding around targets when\n analyzing exomes.",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Set merging approach to use for combining interval inputs",
      "name": "--interval-set-rule",
      "synonyms": "-isr",
      "type": "IntervalSetRule",
      "required": "no",
      "fulltext": "By default, the program will take the UNION of all intervals specified using -L and/or -XL. However, you can\n change this setting for -L, for example if you want to take the INTERSECTION of the sets instead. E.g. to\n perform the analysis only on chromosome 1 exomes, you could specify -L exomes.intervals -L 1 --interval-set-rule\n INTERSECTION. However, it is not possible to modify the merging approach for intervals passed using -XL (they will\n always be merged using UNION).\n\n Note that if you specify both -L and -XL, the -XL interval set will be subtracted from the -L interval set.",
      "defaultValue": "UNION",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": [
        {
          "summary": "Take the union of all intervals",
          "name": "UNION"
        },
        {
          "summary": "Take the intersection of intervals (the subset that overlaps all intervals specified)",
          "name": "INTERSECTION"
        }
      ]
    },
    {
      "summary": "One or more genomic intervals over which to operate",
      "name": "--intervals",
      "synonyms": "-L",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Path to write duplication metrics to.",
      "name": "--metrics-file",
      "synonyms": "-M",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "For tools that shuffle data or write an output, sets the number of reducers. Defaults to 0, which gives one partition per 10MB of input.",
      "name": "--num-reducers",
      "synonyms": "NA",
      "type": "int",
      "required": "no",
      "fulltext": "",
      "defaultValue": "0",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "The maximum offset between two duplicate clusters in order to consider them optical duplicates. This should usually be set to some fairly small number (e.g. 5-10 pixels) unless using later versions of the Illumina pipeline that multiply pixel values by 10, in which case 50-100 is more normal.",
      "name": "--optical-duplicate-pixel-distance",
      "synonyms": "NA",
      "type": "int",
      "required": "no",
      "fulltext": "",
      "defaultValue": "100",
      "minValue": "-Infinity",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "the output bam",
      "name": "--output",
      "synonyms": "-O",
      "type": "String",
      "required": "yes",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "required",
      "options": []
    },
    {
      "summary": "when writing a bam, in single sharded mode this directory to write the temporary intermediate output shards, if not specified .parts/ will be used",
      "name": "--output-shard-tmp-dir",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Name of the program running",
      "name": "--program-name",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Whether to suppress job-summary info on System.err.",
      "name": "--QUIET",
      "synonyms": "NA",
      "type": "Boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Read filters to be applied before analysis",
      "name": "--read-filter",
      "synonyms": "-RF",
      "type": "List[String]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Indices to use for the read inputs. If specified, an index must be provided for every read input and in the same order as the read inputs. If this argument is not specified, the path to the index for each input will be inferred automatically.",
      "name": "--read-index",
      "synonyms": "-read-index",
      "type": "List[GATKPath]",
      "required": "no",
      "fulltext": "",
      "defaultValue": "[]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Regular expression that can be used to parse read names in the incoming SAM file. Read names are parsed to extract three variables: tile/region, x coordinate and y coordinate. These values are used to estimate the rate of optical duplication in order to give a more accurate estimated library size. Set this option to null to disable optical duplicate detection. The regular expression should contain three capture groups for the three variables, in order. It must match the entire read name. Note that if the default regex is specified, a regex match is not actually done, but instead the read name  is split on colon character. For 5 element names, the 3rd, 4th and 5th elements are assumed to be tile, x and y values. For 7 element names (CASAVA 1.8), the 5th, 6th, and 7th elements are assumed to be tile, x and y values.",
      "name": "--read-name-regex",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "\u003coptimized capture of last three \u0027:\u0027 separated fields as numeric values\u003e",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Validation stringency for all SAM/BAM/CRAM/SRA files read by this program.  The default stringency value SILENT can improve performance when processing a BAM file in which variable-length data (read, qualities, tags) do not otherwise need to be decoded.",
      "name": "--read-validation-stringency",
      "synonyms": "-VS",
      "type": "ValidationStringency",
      "required": "no",
      "fulltext": "",
      "defaultValue": "SILENT",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": [
        {
          "summary": "",
          "name": "STRICT"
        },
        {
          "summary": "",
          "name": "LENIENT"
        },
        {
          "summary": "",
          "name": "SILENT"
        }
      ]
    },
    {
      "summary": "Reference sequence",
      "name": "--reference",
      "synonyms": "-R",
      "type": "GATKPath",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "If true do not write duplicates to the output file instead of writing them with appropriate flags set.",
      "name": "--remove-all-duplicates",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "If true do not write optical/sequencing duplicates to the output file instead of writing them with appropriate flags set.",
      "name": "--remove-sequencing-duplicates",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "For tools that write an output, write the output in multiple pieces (shards)",
      "name": "--sharded-output",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "display hidden arguments",
      "name": "--showHidden",
      "synonyms": "-showHidden",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "advanced",
      "options": []
    },
    {
      "summary": "URL of the Spark Master to submit jobs to when using the Spark pipeline runner.",
      "name": "--spark-master",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "local[*]",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Spark verbosity. Overrides --verbosity for Spark-generated logs only. Possible values: {ALL, DEBUG, INFO, WARN, ERROR, FATAL, OFF, TRACE}",
      "name": "--spark-verbosity",
      "synonyms": "NA",
      "type": "String",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Granularity to use when writing a splitting index, one entry will be put into the index every n reads where n is this granularity value. Smaller granularity results in a larger index with more available split points.",
      "name": "--splitting-index-granularity",
      "synonyms": "NA",
      "type": "long",
      "required": "no",
      "fulltext": "",
      "defaultValue": "4096",
      "minValue": "1.0",
      "maxValue": "Infinity",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Temp directory to use.",
      "name": "--tmp-dir",
      "synonyms": "NA",
      "type": "GATKPath",
      "required": "no",
      "fulltext": "",
      "defaultValue": "null",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Treat unsorted files as query-group orderd files. WARNING: This option disables a basic safety check and may result in unexpected behavior if the file is truly unordered",
      "name": "--treat-unsorted-as-querygroup-ordered",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "advanced",
      "options": []
    },
    {
      "summary": "Whether to use the JdkDeflater (as opposed to IntelDeflater)",
      "name": "--use-jdk-deflater",
      "synonyms": "-jdk-deflater",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Whether to use the JdkInflater (as opposed to IntelInflater)",
      "name": "--use-jdk-inflater",
      "synonyms": "-jdk-inflater",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": []
    },
    {
      "summary": "Whether to use NIO or the Hadoop filesystem (default) for reading files. (Note that the Hadoop filesystem is always used for writing files.)",
      "name": "--use-nio",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    },
    {
      "summary": "Control verbosity of logging.",
      "name": "--verbosity",
      "synonyms": "-verbosity",
      "type": "LogLevel",
      "required": "no",
      "fulltext": "",
      "defaultValue": "INFO",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "common",
      "options": [
        {
          "summary": "",
          "name": "ERROR"
        },
        {
          "summary": "",
          "name": "WARNING"
        },
        {
          "summary": "",
          "name": "INFO"
        },
        {
          "summary": "",
          "name": "DEBUG"
        }
      ]
    },
    {
      "summary": "display the version number for this tool",
      "name": "--version",
      "synonyms": "NA",
      "type": "boolean",
      "required": "no",
      "fulltext": "",
      "defaultValue": "false",
      "minValue": "NA",
      "maxValue": "NA",
      "minRecValue": "NA",
      "maxRecValue": "NA",
      "kind": "optional",
      "options": []
    }
  ],
  "description": "MarkDuplicates on Spark\n\n \u003cp\u003eThis is a Spark implementation of \u003ca href\u003d\u0027https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_markduplicates_MarkDuplicates.php\u0027\u003ePicard MarkDuplicates\u003c/a\u003e that allows the tool to be run in parallel on multiple cores on a local machine or multiple machines on a Spark cluster while still matching the output of the non-Spark Picard version of the tool. Since the tool requires holding all of the readnames in memory while it groups read information, machine configuration and starting sort-order impact tool performance. \u003c/p\u003e\n\n Here are some differences of note between MarkDuplicatesSpark and Picard MarkDuplicates.\n\n \u003cul\u003e\n  \u003cli\u003eMarkDuplicatesSpark processing can replace both the MarkDuplicates and SortSam steps of the Best Practices \u003ca href\u003d\"https://software.broadinstitute.org/gatk/documentation/article?id\u003d7899#2\"\u003esingle sample pipeline\u003c/a\u003e. After flagging duplicate sets, the tool automatically coordinate-sorts the records. It is recommended to subsequently run SetNmMdAndUqTags before running BQSR. \u003c/li\u003e\n  \u003cli\u003eThe tool is optimized to run on queryname-grouped alignments (that is, all reads with the same queryname are together in the input file). If provided coordinate-sorted alignments, the tool will spend additional time first queryname sorting the reads internally. This can result in the tool being up to 2x slower processing under some circumstances.\u003c/li\u003e\n  \u003cli\u003eDue to MarkDuplicatesSpark queryname-sorting coordinate-sorted inputs internally at the start, the tool produces identical results regardless of the input sort-order. That is, it will flag duplicates sets that include secondary, and supplementary and unmapped mate records no matter the sort-order of the input. This differs from how Picard MarkDuplicates behaves given the differently sorted inputs. \u003c/li\u003e\n  \u003cli\u003eCollecting duplicate metrics slows down performance and thus the metrics collection is optional and must be specified for the Spark version of the tool with \u0027-M\u0027. It is possible to collect the metrics with the standalone Picard tool \u003ca href\u003d\u0027https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_markduplicates_EstimateLibraryComplexity.php\u0027\u003eEstimateLibraryComplexity\u003c/a\u003e.\u003c/li\u003e\n  \u003cli\u003eMarkDuplicatesSpark is optimized to run locally on a single machine by leveraging core parallelism that MarkDuplicates and SortSam cannot. It will typically run faster than MarkDuplicates and SortSam by a factor of 15% over the same data at 2 cores and will scale linearly to upwards of 16 cores. This means MarkDuplicatesSpark, even without access to a Spark cluster, is faster than MarkDuplicates.\u003c/li\u003e\n  \u003cli\u003eMarkDuplicatesSpark can be run with multiple input bams. If this is the case all of the inputs must be a mix queryname-grouped or queryname sorted.\u003c/li\u003e\n \u003c/ul\u003e\n\n \u003cp\u003eFor a typical 30x coverage WGS BAM, we recommend running on a machine with at least 16 GB. Memory usage scales with library complexity and the tool will need more memory for larger or more complex data. If the tool is running slowly it is possible Spark is running out of memory and is spilling data to disk excessively. If this is the case then increasing the memory available to the tool should yield speedup to a threshold; otherwise, increasing memory should have no effect beyond that threshold. \u003c/p\u003e\n\n \u003cp\u003e Note that this tool does not support UMI based duplicate marking. \u003c/p\u003e\n\n \u003cp\u003eSee \u003ca href\u003d\u0027https://software.broadinstitute.org/gatk/documentation/tooldocs/current/picard_sam_markduplicates_MarkDuplicates.php\u0027\u003eMarkDuplicates documentation\u003c/a\u003e for details on tool features and background information. \u003c/p\u003e\n\n \u003ch3\u003eUsage examples\u003c/h3\u003e\n Provide queryname-grouped reads to MarkDuplicatesSpark\n     \u003cpre\u003e\n      gatk MarkDuplicatesSpark \\\n            -I input.bam \\\n            -O marked_duplicates.bam\n     \u003c/pre\u003e\n\n Additionally produce estimated library complexity metrics\n     \u003cpre\u003e\n     gatk MarkDuplicatesSpark \\\n             -I input.bam \\\n             -O marked_duplicates.bam \\\n             -M marked_dup_metrics.txt\n\n     \u003c/pre\u003e\n\n\n MarkDuplicatesSpark run locally specifying the removal of sequencing duplicates\n     \u003cpre\u003e\n       gatk MarkDuplicatesSpark \\\n            -I input.bam \\\n            -O marked_duplicates.bam \\\n            --remove-sequencing-duplicates\n     \u003c/pre\u003e\n\n MarkDuplicatesSpark run locally tagging OpticalDuplicates using the \"DT\" attribute for reads\n     \u003cpre\u003e\n       gatk MarkDuplicatesSpark \\\n            -I input.bam \\\n            -O marked_duplicates.bam \\\n            --duplicate-tagging-policy OpticalOnly\n     \u003c/pre\u003e\n\n  MarkDuplicates run locally specifying the core input. Note if \u0027spark.executor.cores\u0027 is unset, Spark will use all available cores on the machine.\n     \u003cpre\u003e\n       gatk MarkDuplicatesSpark \\\n            -I input.bam \\\n            -O marked_duplicates.bam \\\n            -M marked_dup_metrics.txt \\\n            --conf \u0027spark.executor.cores\u003d5\u0027\n     \u003c/pre\u003e\n\n  MarkDuplicates run on a Spark cluster of five executors  and with eight executor cores\n     \u003cpre\u003e\n       gatk MarkDuplicatesSpark \\\n            -I input.bam \\\n            -O marked_duplicates.bam \\\n            -M marked_dup_metrics.txt \\\n            -- \\\n            --spark-runner SPARK \\\n            --spark-master MASTER_URL \\\n            --num-executors 5 \\\n            --executor-cores 8\n     \u003c/pre\u003e\n\n    Please see\n    \u003ca href\u003d\u0027http://broadinstitute.github.io/picard/picard-metric-definitions.html#DuplicationMetrics\u0027\u003ePicard DuplicationMetrics\u003c/a\u003e\n    for detailed explanations of the output metrics.\n    \u003chr /\u003e\n\n \u003ch3\u003eNotes\u003c/h3\u003e\n \u003col\u003e\n     \u003cli\u003eThis Spark tool requires a significant amount of disk operations. Run with both the input data and outputs on high throughput SSDs when possible. When pipelining this tool on Google Compute Engine instances, for best performance requisition machines with LOCAL SSDs.  \u003c/li\u003e\n     \u003cli\u003eFurthermore, we recommend explicitly setting the Spark temp directory to an available SSD when running this in local mode by adding the argument --conf \u0027spark.local.dir\u003d/PATH/TO/TEMP/DIR\u0027. See \u003ca href\u003d\"https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2019-02-11-2018-08-12/23441-MarkDuplicateSpark-is-slower-than-normal-MarkDuplicates\"\u003ethis forum discussion\u003c/a\u003e for details.\u003c/li\u003e\n \u003c/ol\u003e",
  "name": "MarkDuplicatesSpark",
  "group": "Read Data Manipulation",
  "beta": false,
  "experimental": false
}